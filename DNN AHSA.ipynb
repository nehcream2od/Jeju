{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_preprocessed.csv')\n",
    "test = pd.read_csv('test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:int(len(train)*0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('target', axis=1)\n",
    "y_train = train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, shuffle=True, random_state=119)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# definition DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(add_layer, first_layer_size, layer_size, activation, dropout_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(first_layer_size, activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    for i in range(add_layer):\n",
    "        model.add(tf.keras.layers.Dense(layer_size, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sherpa Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sherpa\n",
    "import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
    "import tempfile\n",
    "\n",
    "# parameter\n",
    "\n",
    "parameters = [sherpa.Continuous('learning_rate', [1e-4, 1e-2], 'log'),\n",
    "              sherpa.Choice('batch', [32, 64, 128]),\n",
    "              sherpa.Discrete('add_layer', [1, 5]),\n",
    "              sherpa.Choice('first_layer_size', [8, 16, 32, 64]),\n",
    "              sherpa.Choice('layer_size', [8, 16, 32, 64]),       \n",
    "              sherpa.Choice('activation', ['relu', 'swish']),\n",
    "              sherpa.Continuous('dropout_rate', [0, 0.5])]\n",
    "algorithm = alg = sherpa.algorithms.SuccessiveHalving(r=1, R=9, eta=3, s=0, max_finished_configs=1)\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=algorithm,\n",
    "                     lower_is_better=False,\n",
    "                     disable_dashboard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t1\n",
      "Epochs:\t0 to 1\n",
      "Parameters:{'learning_rate': 0.0025015748328547117, 'batch': 64, 'add_layer': 4, 'first_layer_size': 64, 'layer_size': 64, 'activation': 'swish', 'dropout_rate': 0.07761866886538371, 'resource': 1, 'rung': 0, 'load_from': '', 'save_to': '1'}\n",
      "\n",
      "Creating new model for trial 1...\n",
      "\n",
      "661/661 [==============================] - 2s 3ms/step - loss: 215.9115 - val_loss: 121.3977\n",
      "147/147 [==============================] - 0s 815us/step\n",
      "MAE loss:  8.772959657650407\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\1\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t2\n",
      "Epochs:\t0 to 1\n",
      "Parameters:{'learning_rate': 0.0031616589003147216, 'batch': 128, 'add_layer': 4, 'first_layer_size': 16, 'layer_size': 8, 'activation': 'relu', 'dropout_rate': 0.29994655900127837, 'resource': 1, 'rung': 0, 'load_from': '', 'save_to': '2'}\n",
      "\n",
      "Creating new model for trial 2...\n",
      "\n",
      "330/330 [==============================] - 1s 2ms/step - loss: 1007.3499 - val_loss: 416.2033\n",
      "147/147 [==============================] - 0s 758us/step\n",
      "MAE loss:  16.551137286518443\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\2\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t3\n",
      "Epochs:\t0 to 1\n",
      "Parameters:{'learning_rate': 0.0009310925304722731, 'batch': 64, 'add_layer': 1, 'first_layer_size': 32, 'layer_size': 8, 'activation': 'swish', 'dropout_rate': 0.49898968912924996, 'resource': 1, 'rung': 0, 'load_from': '', 'save_to': '3'}\n",
      "\n",
      "Creating new model for trial 3...\n",
      "\n",
      "661/661 [==============================] - 1s 1ms/step - loss: 961.9319 - val_loss: 269.3282\n",
      "147/147 [==============================] - 0s 743us/step\n",
      "MAE loss:  13.082607614492062\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\3\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t4\n",
      "Epochs:\t1 to 4\n",
      "Parameters:{'learning_rate': 0.0025015748328547117, 'batch': 64, 'add_layer': 4, 'first_layer_size': 64, 'layer_size': 64, 'activation': 'swish', 'dropout_rate': 0.07761866886538371, 'save_to': '4', 'resource': 3, 'rung': 1, 'load_from': '1'}\n",
      "\n",
      "Loading model from:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\1 ...\n",
      "\n",
      "Epoch 2/2\n",
      "661/661 [==============================] - 3s 3ms/step - loss: 131.8630 - val_loss: 110.4004\n",
      "147/147 [==============================] - 0s 891us/step\n",
      "MAE loss:  8.198411973168728\n",
      "Epoch 3/3\n",
      "661/661 [==============================] - 2s 2ms/step - loss: 118.3669 - val_loss: 96.2551\n",
      "147/147 [==============================] - 0s 920us/step\n",
      "MAE loss:  7.730054396966426\n",
      "Epoch 4/4\n",
      "661/661 [==============================] - 2s 2ms/step - loss: 109.8590 - val_loss: 92.1643\n",
      "147/147 [==============================] - 0s 924us/step\n",
      "MAE loss:  7.597824118584585\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\4\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t5\n",
      "Epochs:\t0 to 1\n",
      "Parameters:{'learning_rate': 0.00035838184068751646, 'batch': 64, 'add_layer': 2, 'first_layer_size': 8, 'layer_size': 8, 'activation': 'relu', 'dropout_rate': 0.36325886757768816, 'resource': 1, 'rung': 0, 'load_from': '', 'save_to': '5'}\n",
      "\n",
      "Creating new model for trial 5...\n",
      "\n",
      "661/661 [==============================] - 1s 1ms/step - loss: 1665.9237 - val_loss: 493.8914\n",
      "147/147 [==============================] - 0s 781us/step\n",
      "MAE loss:  17.990584242450385\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t6\n",
      "Epochs:\t0 to 1\n",
      "Parameters:{'learning_rate': 0.0009557204074155853, 'batch': 128, 'add_layer': 2, 'first_layer_size': 8, 'layer_size': 32, 'activation': 'relu', 'dropout_rate': 0.022005292319160263, 'resource': 1, 'rung': 0, 'load_from': '', 'save_to': '6'}\n",
      "\n",
      "Creating new model for trial 6...\n",
      "\n",
      "330/330 [==============================] - 2s 3ms/step - loss: 646.3953 - val_loss: 176.0035\n",
      "147/147 [==============================] - 0s 810us/step\n",
      "MAE loss:  10.800274556467047\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\6\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t7\n",
      "Epochs:\t0 to 1\n",
      "Parameters:{'learning_rate': 0.007696661565658455, 'batch': 32, 'add_layer': 4, 'first_layer_size': 32, 'layer_size': 8, 'activation': 'swish', 'dropout_rate': 0.09623561584607077, 'resource': 1, 'rung': 0, 'load_from': '', 'save_to': '7'}\n",
      "\n",
      "Creating new model for trial 7...\n",
      "\n",
      "1322/1322 [==============================] - 3s 1ms/step - loss: 310.0114 - val_loss: 151.7630\n",
      "147/147 [==============================] - 0s 790us/step\n",
      "MAE loss:  9.970090142821617\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\7\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t8\n",
      "Epochs:\t1 to 4\n",
      "Parameters:{'learning_rate': 0.007696661565658455, 'batch': 32, 'add_layer': 4, 'first_layer_size': 32, 'layer_size': 8, 'activation': 'swish', 'dropout_rate': 0.09623561584607077, 'save_to': '8', 'resource': 3, 'rung': 1, 'load_from': '7'}\n",
      "\n",
      "Loading model from:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\7 ...\n",
      "\n",
      "Epoch 2/2\n",
      "1322/1322 [==============================] - 3s 1ms/step - loss: 170.3010 - val_loss: 108.5553\n",
      "147/147 [==============================] - 0s 838us/step\n",
      "MAE loss:  8.27059115982218\n",
      "Epoch 3/3\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 138.5479 - val_loss: 99.3324\n",
      "147/147 [==============================] - 0s 827us/step\n",
      "MAE loss:  7.917717166172094\n",
      "Epoch 4/4\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 124.9549 - val_loss: 90.3721\n",
      "147/147 [==============================] - 0s 847us/step\n",
      "MAE loss:  7.501959194684728\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\8\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t9\n",
      "Epochs:\t0 to 1\n",
      "Parameters:{'learning_rate': 0.00025998737821070705, 'batch': 64, 'add_layer': 4, 'first_layer_size': 16, 'layer_size': 64, 'activation': 'relu', 'dropout_rate': 0.44549068016975285, 'resource': 1, 'rung': 0, 'load_from': '', 'save_to': '9'}\n",
      "\n",
      "Creating new model for trial 9...\n",
      "\n",
      "661/661 [==============================] - 2s 2ms/step - loss: 706.7488 - val_loss: 481.6839\n",
      "147/147 [==============================] - 0s 767us/step\n",
      "MAE loss:  17.870293270826643\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\9\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t10\n",
      "Epochs:\t0 to 1\n",
      "Parameters:{'learning_rate': 0.0037032776418304105, 'batch': 32, 'add_layer': 2, 'first_layer_size': 8, 'layer_size': 64, 'activation': 'swish', 'dropout_rate': 0.4189505641667687, 'resource': 1, 'rung': 0, 'load_from': '', 'save_to': '10'}\n",
      "\n",
      "Creating new model for trial 10...\n",
      "\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 272.9706 - val_loss: 146.4215\n",
      "147/147 [==============================] - 0s 788us/step\n",
      "MAE loss:  9.804489704052573\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\10\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t11\n",
      "Epochs:\t1 to 4\n",
      "Parameters:{'learning_rate': 0.0037032776418304105, 'batch': 32, 'add_layer': 2, 'first_layer_size': 8, 'layer_size': 64, 'activation': 'swish', 'dropout_rate': 0.4189505641667687, 'save_to': '11', 'resource': 3, 'rung': 1, 'load_from': '10'}\n",
      "\n",
      "Loading model from:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\10 ...\n",
      "\n",
      "Epoch 2/2\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 187.6142 - val_loss: 140.3114\n",
      "147/147 [==============================] - 0s 813us/step\n",
      "MAE loss:  9.526866286523997\n",
      "Epoch 3/3\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 173.7816 - val_loss: 131.6262\n",
      "147/147 [==============================] - 0s 821us/step\n",
      "MAE loss:  9.236324738665166\n",
      "Epoch 4/4\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 168.7438 - val_loss: 131.3286\n",
      "147/147 [==============================] - 0s 816us/step\n",
      "MAE loss:  9.19952284189449\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\11\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\11\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\11\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Trial:\t12\n",
      "Epochs:\t4 to 13\n",
      "Parameters:{'learning_rate': 0.007696661565658455, 'batch': 32, 'add_layer': 4, 'first_layer_size': 32, 'layer_size': 8, 'activation': 'swish', 'dropout_rate': 0.09623561584607077, 'save_to': '12', 'resource': 9, 'rung': 2, 'load_from': '8'}\n",
      "\n",
      "Loading model from:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\8 ...\n",
      "\n",
      "Epoch 5/5\n",
      "1322/1322 [==============================] - 3s 1ms/step - loss: 117.9291 - val_loss: 90.2002\n",
      "147/147 [==============================] - 0s 852us/step\n",
      "MAE loss:  7.5130105795428275\n",
      "Epoch 6/6\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 111.0453 - val_loss: 88.2421\n",
      "147/147 [==============================] - 0s 856us/step\n",
      "MAE loss:  7.398179975482871\n",
      "Epoch 7/7\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 107.4580 - val_loss: 85.9058\n",
      "147/147 [==============================] - 0s 896us/step\n",
      "MAE loss:  7.3070362521860766\n",
      "Epoch 8/8\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 106.2635 - val_loss: 84.2502\n",
      "147/147 [==============================] - 0s 866us/step\n",
      "MAE loss:  7.258820676134272\n",
      "Epoch 9/9\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 104.1937 - val_loss: 86.4892\n",
      "147/147 [==============================] - 0s 856us/step\n",
      "MAE loss:  7.315299849061954\n",
      "Epoch 10/10\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 101.4606 - val_loss: 83.9964\n",
      "147/147 [==============================] - 0s 886us/step\n",
      "MAE loss:  7.2286423886395275\n",
      "Epoch 11/11\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 100.4158 - val_loss: 83.4750\n",
      "147/147 [==============================] - 0s 877us/step\n",
      "MAE loss:  7.214981903780374\n",
      "Epoch 12/12\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 100.8258 - val_loss: 83.2552\n",
      "147/147 [==============================] - 0s 858us/step\n",
      "MAE loss:  7.2305031074254575\n",
      "Epoch 13/13\n",
      "1322/1322 [==============================] - 2s 1ms/step - loss: 99.8326 - val_loss: 83.2007\n",
      "147/147 [==============================] - 0s 859us/step\n",
      "MAE loss:  7.171454376081161\n",
      "Saving model at:  C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\12\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\12\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehcr\\AppData\\Local\\Temp\\tmpbvez4c3u\\12\\assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "for trial in study:\n",
    "    # Getting number of training epochs\n",
    "    initial_epoch = {1: 0, 3: 1, 9: 4}[trial.parameters['resource']]\n",
    "    epochs = trial.parameters['resource'] + initial_epoch\n",
    "\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Trial:\\t{trial.id}\\nEpochs:\\t{initial_epoch} to {epochs}\\nParameters:{trial.parameters}\\n\")\n",
    "\n",
    "    if trial.parameters['load_from'] == \"\":\n",
    "        print(f\"Creating new model for trial {trial.id}...\\n\")\n",
    "\n",
    "        # Get hyperparameters\n",
    "        learning_rate = trial.parameters['learning_rate']\n",
    "        batch_size = trial.parameters['batch']\n",
    "        add_layer = trial.parameters['add_layer']\n",
    "        first_layer_size = trial.parameters['first_layer_size']\n",
    "        layer_size = trial.parameters['layer_size']\n",
    "        activation = trial.parameters['activation']\n",
    "        dropout_rate = trial.parameters['dropout_rate']\n",
    "\n",
    "        # Create model\n",
    "        model = DNN(add_layer, first_layer_size, layer_size, activation, dropout_rate)\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "        \n",
    "    else:\n",
    "        print(f\"Loading model from: \", os.path.join(model_dir, trial.parameters['load_from']), \"...\\n\")\n",
    "\n",
    "        # Loading model\n",
    "        model = tf.keras.models.load_model(os.path.join(model_dir, trial.parameters['load_from']))\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    for i in range(initial_epoch, epochs):\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            model.fit(\n",
    "        X_train, y_train,\n",
    "        initial_epoch=i,\n",
    "        epochs = i+1,\n",
    "        steps_per_epoch=len(X_train) / batch_size,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        validation_steps=len(X_val) / batch_size,\n",
    "        shuffle=True)\n",
    "        y_pred = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "        print(\"MAE loss: \", mae)\n",
    "        study.add_observation(trial=trial, iteration=i,\n",
    "                              objective=-mae,\n",
    "                              context={'loss': mae})\n",
    "        \n",
    "\n",
    "    study.finalize(trial=trial)\n",
    "    print(f\"Saving model at: \", os.path.join(model_dir, trial.parameters['save_to']))\n",
    "    model.save(os.path.join(model_dir, trial.parameters['save_to']))\n",
    "\n",
    "    study.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serached hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* learning rate : 0.005, 0.007\n",
    "* batch : 64, 32\n",
    "* add_layer : 3, 4\n",
    "* first_layer_size : 64, 32\n",
    "* layer_size : 32, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seed hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 시드고정\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def call_back_set(name, epoch, batch_size):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=earlystop_patience)\n",
    "\n",
    "    if os.path.exists(f'./check') == False:\n",
    "        os.mkdir(f'./check')\n",
    "\n",
    "    filename = f'./check/{name}-{epoch}-{batch_size}.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filename,\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,\n",
    "                                 mode='auto'\n",
    "                                 )\n",
    "    return [early_stopping, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_val, y_val, name, epoch, batch_size, learning_rate, verbose = 1):\n",
    "    model = DNN(3, 64, 32, 'swish', 0.1)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history1 = model.fit(\n",
    "            x_train, y_train,\n",
    "            epochs = epoch,\n",
    "            steps_per_epoch=len(x_train) / batch_size,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_val, y_val),\n",
    "            validation_steps=len(x_val) / batch_size,\n",
    "            shuffle=False,\n",
    "            callbacks=call_back_set(name, epoch, batch_size),\n",
    "            verbose=verbose)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "batch = 64\n",
    "learning_rate = 0.005\n",
    "earlystop_patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66079/66110 [============================>.] - ETA: 0s - loss: 84.7983\n",
      "Epoch 1: val_loss improved from inf to 67.30554, saving model to ./check\\DNN-100-64.h5\n",
      "66110/66110 [==============================] - 105s 2ms/step - loss: 84.7936 - val_loss: 67.3055\n",
      "Epoch 2/100\n",
      "66111/66110 [==============================] - ETA: 0s - loss: 73.1716\n",
      "Epoch 2: val_loss improved from 67.30554 to 63.93384, saving model to ./check\\DNN-100-64.h5\n",
      "66110/66110 [==============================] - 104s 2ms/step - loss: 73.1716 - val_loss: 63.9338\n",
      "Epoch 3/100\n",
      "66082/66110 [============================>.] - ETA: 0s - loss: 71.3407\n",
      "Epoch 3: val_loss improved from 63.93384 to 63.38778, saving model to ./check\\DNN-100-64.h5\n",
      "66110/66110 [==============================] - 104s 2ms/step - loss: 71.3392 - val_loss: 63.3878\n",
      "Epoch 4/100\n",
      "66078/66110 [============================>.] - ETA: 0s - loss: 70.7956\n",
      "Epoch 4: val_loss improved from 63.38778 to 62.82922, saving model to ./check\\DNN-100-64.h5\n",
      "66110/66110 [==============================] - 104s 2ms/step - loss: 70.7943 - val_loss: 62.8292\n",
      "Epoch 5/100\n",
      "66105/66110 [============================>.] - ETA: 0s - loss: 70.5176\n",
      "Epoch 5: val_loss improved from 62.82922 to 61.47366, saving model to ./check\\DNN-100-64.h5\n",
      "66110/66110 [==============================] - 105s 2ms/step - loss: 70.5168 - val_loss: 61.4737\n",
      "Epoch 6/100\n",
      "66084/66110 [============================>.] - ETA: 0s - loss: 70.5220\n",
      "Epoch 6: val_loss did not improve from 61.47366\n",
      "66110/66110 [==============================] - 105s 2ms/step - loss: 70.5194 - val_loss: 61.5007\n",
      "Epoch 7/100\n",
      "66109/66110 [============================>.] - ETA: 0s - loss: 69.0541\n",
      "Epoch 7: val_loss did not improve from 61.47366\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 69.0538 - val_loss: 61.9172\n",
      "Epoch 8/100\n",
      "66103/66110 [============================>.] - ETA: 0s - loss: 72.5634\n",
      "Epoch 8: val_loss did not improve from 61.47366\n",
      "66110/66110 [==============================] - 107s 2ms/step - loss: 72.5633 - val_loss: 71.8407\n",
      "Epoch 9/100\n",
      "66083/66110 [============================>.] - ETA: 0s - loss: 70.9499\n",
      "Epoch 9: val_loss did not improve from 61.47366\n",
      "66110/66110 [==============================] - 108s 2ms/step - loss: 70.9477 - val_loss: 62.1752\n",
      "Epoch 10/100\n",
      "66080/66110 [============================>.] - ETA: 0s - loss: 70.5863\n",
      "Epoch 10: val_loss did not improve from 61.47366\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 70.5851 - val_loss: 61.8452\n",
      "Epoch 11/100\n",
      "66081/66110 [============================>.] - ETA: 0s - loss: 71.8593\n",
      "Epoch 11: val_loss did not improve from 61.47366\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 71.8574 - val_loss: 64.1554\n",
      "Epoch 12/100\n",
      "66095/66110 [============================>.] - ETA: 0s - loss: 71.6085\n",
      "Epoch 12: val_loss improved from 61.47366 to 61.21226, saving model to ./check\\DNN-100-64.h5\n",
      "66110/66110 [==============================] - 105s 2ms/step - loss: 71.6064 - val_loss: 61.2123\n",
      "Epoch 13/100\n",
      "66090/66110 [============================>.] - ETA: 0s - loss: 71.7567\n",
      "Epoch 13: val_loss did not improve from 61.21226\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 71.7550 - val_loss: 63.6809\n",
      "Epoch 14/100\n",
      "66108/66110 [============================>.] - ETA: 0s - loss: 70.3557\n",
      "Epoch 14: val_loss did not improve from 61.21226\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 70.3553 - val_loss: 62.2651\n",
      "Epoch 15/100\n",
      "66085/66110 [============================>.] - ETA: 0s - loss: 71.4741\n",
      "Epoch 15: val_loss improved from 61.21226 to 60.50599, saving model to ./check\\DNN-100-64.h5\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 71.4706 - val_loss: 60.5060\n",
      "Epoch 16/100\n",
      "66090/66110 [============================>.] - ETA: 0s - loss: 88.7522\n",
      "Epoch 16: val_loss did not improve from 60.50599\n",
      "66110/66110 [==============================] - 107s 2ms/step - loss: 88.7441 - val_loss: 62.3468\n",
      "Epoch 17/100\n",
      "66100/66110 [============================>.] - ETA: 0s - loss: 71.0751\n",
      "Epoch 17: val_loss did not improve from 60.50599\n",
      "66110/66110 [==============================] - 107s 2ms/step - loss: 71.0746 - val_loss: 64.5355\n",
      "Epoch 18/100\n",
      "66088/66110 [============================>.] - ETA: 0s - loss: 71.4694\n",
      "Epoch 18: val_loss did not improve from 60.50599\n",
      "66110/66110 [==============================] - 107s 2ms/step - loss: 71.4684 - val_loss: 62.4711\n",
      "Epoch 19/100\n",
      "66111/66110 [==============================] - ETA: 0s - loss: 74.1725\n",
      "Epoch 19: val_loss did not improve from 60.50599\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 74.1725 - val_loss: 60.5808\n",
      "Epoch 20/100\n",
      "66093/66110 [============================>.] - ETA: 0s - loss: 71.7713\n",
      "Epoch 20: val_loss did not improve from 60.50599\n",
      "66110/66110 [==============================] - 105s 2ms/step - loss: 71.7696 - val_loss: 60.5660\n",
      "Epoch 21/100\n",
      "66084/66110 [============================>.] - ETA: 0s - loss: 70.5689\n",
      "Epoch 21: val_loss did not improve from 60.50599\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 70.5652 - val_loss: 60.6179\n",
      "Epoch 22/100\n",
      "66088/66110 [============================>.] - ETA: 0s - loss: 74.5927\n",
      "Epoch 22: val_loss did not improve from 60.50599\n",
      "66110/66110 [==============================] - 105s 2ms/step - loss: 74.5900 - val_loss: 63.1252\n",
      "Epoch 23/100\n",
      "66108/66110 [============================>.] - ETA: 0s - loss: 72.9680\n",
      "Epoch 23: val_loss improved from 60.50599 to 59.90529, saving model to ./check\\DNN-100-64.h5\n",
      "66110/66110 [==============================] - 105s 2ms/step - loss: 72.9674 - val_loss: 59.9053\n",
      "Epoch 24/100\n",
      "66097/66110 [============================>.] - ETA: 0s - loss: 77.2417\n",
      "Epoch 24: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 77.2392 - val_loss: 61.9532\n",
      "Epoch 25/100\n",
      "66088/66110 [============================>.] - ETA: 0s - loss: 73.5701\n",
      "Epoch 25: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 104s 2ms/step - loss: 73.5682 - val_loss: 63.3021\n",
      "Epoch 26/100\n",
      "66086/66110 [============================>.] - ETA: 0s - loss: 76.5818\n",
      "Epoch 26: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 106s 2ms/step - loss: 76.5771 - val_loss: 61.3920\n",
      "Epoch 27/100\n",
      "66077/66110 [============================>.] - ETA: 0s - loss: 759.8825\n",
      "Epoch 27: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 101s 2ms/step - loss: 759.5303 - val_loss: 65.7431\n",
      "Epoch 28/100\n",
      "66097/66110 [============================>.] - ETA: 0s - loss: 99.6947\n",
      "Epoch 28: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 100s 2ms/step - loss: 99.6891 - val_loss: 68.6091\n",
      "Epoch 29/100\n",
      "66080/66110 [============================>.] - ETA: 0s - loss: 838.0132\n",
      "Epoch 29: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 101s 2ms/step - loss: 837.6791 - val_loss: 94.1254\n",
      "Epoch 30/100\n",
      "66103/66110 [============================>.] - ETA: 0s - loss: 602.9678\n",
      "Epoch 30: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 102s 2ms/step - loss: 602.9054 - val_loss: 90.4836\n",
      "Epoch 31/100\n",
      "66078/66110 [============================>.] - ETA: 0s - loss: 124.2284\n",
      "Epoch 31: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 104s 2ms/step - loss: 124.2014 - val_loss: 63.3229\n",
      "Epoch 32/100\n",
      "66089/66110 [============================>.] - ETA: 0s - loss: 606.1230\n",
      "Epoch 32: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 100s 2ms/step - loss: 605.9614 - val_loss: 85.1750\n",
      "Epoch 33/100\n",
      "66097/66110 [============================>.] - ETA: 0s - loss: 123.2148\n",
      "Epoch 33: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 100s 2ms/step - loss: 123.2041 - val_loss: 67.8236\n",
      "Epoch 34/100\n",
      "66107/66110 [============================>.] - ETA: 0s - loss: 2356.4407\n",
      "Epoch 34: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 99s 1ms/step - loss: 2356.3071 - val_loss: 67.5909\n",
      "Epoch 35/100\n",
      "66089/66110 [============================>.] - ETA: 0s - loss: 98.7132\n",
      "Epoch 35: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 99s 1ms/step - loss: 98.7038 - val_loss: 64.2821\n",
      "Epoch 36/100\n",
      "66083/66110 [============================>.] - ETA: 0s - loss: 166.3368\n",
      "Epoch 36: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 99s 2ms/step - loss: 166.2944 - val_loss: 62.8824\n",
      "Epoch 37/100\n",
      "66098/66110 [============================>.] - ETA: 0s - loss: 126.4863\n",
      "Epoch 37: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 100s 2ms/step - loss: 126.4756 - val_loss: 68.3375\n",
      "Epoch 38/100\n",
      "66098/66110 [============================>.] - ETA: 0s - loss: 1507.2463\n",
      "Epoch 38: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 98s 1ms/step - loss: 1506.9680 - val_loss: 75.4324\n",
      "Epoch 39/100\n",
      "66080/66110 [============================>.] - ETA: 0s - loss: 178.8064\n",
      "Epoch 39: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 100s 2ms/step - loss: 178.7571 - val_loss: 65.9594\n",
      "Epoch 40/100\n",
      "66078/66110 [============================>.] - ETA: 0s - loss: 360.1606\n",
      "Epoch 40: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 99s 1ms/step - loss: 360.0164 - val_loss: 64.6030\n",
      "Epoch 41/100\n",
      "66109/66110 [============================>.] - ETA: 0s - loss: 127.1649\n",
      "Epoch 41: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 99s 1ms/step - loss: 127.1634 - val_loss: 67.8430\n",
      "Epoch 42/100\n",
      "66083/66110 [============================>.] - ETA: 0s - loss: 116.4426\n",
      "Epoch 42: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 99s 1ms/step - loss: 116.4235 - val_loss: 66.3106\n",
      "Epoch 43/100\n",
      "66106/66110 [============================>.] - ETA: 0s - loss: 136.1502\n",
      "Epoch 43: val_loss did not improve from 59.90529\n",
      "66110/66110 [==============================] - 99s 2ms/step - loss: 136.1456 - val_loss: 67.7618\n"
     ]
    }
   ],
   "source": [
    " # DNN 모델 훈련\n",
    "DNN_model = train(X_train, y_train, X_val, y_val, f'DNN', epoch,\n",
    "                    batch, learning_rate)\n",
    "DNN_model.load_weights(f'./check/DNN-{epoch}-{batch}.h5')\n",
    "\n",
    "if os.path.exists(f'./model') == False:\n",
    "    os.mkdir(f'./model')\n",
    "\n",
    "# 모델 저장\n",
    "DNN_model.save(f'./model/DNN-{epoch}-{batch}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9102/9102 [==============================] - 6s 637us/step\n"
     ]
    }
   ],
   "source": [
    "pred = DNN_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000000</td>\n",
       "      <td>27.943232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_000001</td>\n",
       "      <td>49.377777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_000002</td>\n",
       "      <td>55.243835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_000003</td>\n",
       "      <td>28.966661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_000004</td>\n",
       "      <td>42.692909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291236</th>\n",
       "      <td>TEST_291236</td>\n",
       "      <td>44.354324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291237</th>\n",
       "      <td>TEST_291237</td>\n",
       "      <td>51.567940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291238</th>\n",
       "      <td>TEST_291238</td>\n",
       "      <td>29.182550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291239</th>\n",
       "      <td>TEST_291239</td>\n",
       "      <td>28.380165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291240</th>\n",
       "      <td>TEST_291240</td>\n",
       "      <td>45.490238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291241 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     target\n",
       "0       TEST_000000  27.943232\n",
       "1       TEST_000001  49.377777\n",
       "2       TEST_000002  55.243835\n",
       "3       TEST_000003  28.966661\n",
       "4       TEST_000004  42.692909\n",
       "...             ...        ...\n",
       "291236  TEST_291236  44.354324\n",
       "291237  TEST_291237  51.567940\n",
       "291238  TEST_291238  29.182550\n",
       "291239  TEST_291239  28.380165\n",
       "291240  TEST_291240  45.490238\n",
       "\n",
       "[291241 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 저장\n",
    "sample_submission = pd.read_csv('./sample_submission.csv')\n",
    "sample_submission['target'] = pred\n",
    "sample_submission.to_csv(\"./submit.csv\", index = False)\n",
    "\n",
    "sample_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deeplearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b4c3ec0e12fd879c7a79621ce69efc8e5dffb38beade75d33d8e63cc2d1166b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
